# ===== RNN DEPLOYMENT COST ANALYSIS REPORT =====

**Project:** Next Word Prediction RNN
**Cloud Provider:** AWS
**Region:** us-east-1
**Analysis Date:** October 30, 2025

---

## MODEL SPECIFICATIONS

- **Architecture:** LSTM (2 layers)
- **Parameters:** ~5.3M
- **Model Size:** 20.35 MB
- **Dataset Size:** 5.0 GB

---

## INFRASTRUCTURE SPECIFICATIONS

- **Training Instance:** `g4dn.xlarge` (4 vCPU, 16 GB RAM, 1x T4 GPU)
- **Inference Instance:** `c5.large` (2 vCPU, 4 GB RAM)
- **Storage:** 50 GB EBS SSD (gp3)

---

## COST BREAKDOWN

_(Costs generated by `cost_model.py`)_

### 1. TRAINING COSTS (One-time)

- **Compute:** $1.32 (2.5 hours × $0.526/hour)
- **Storage:** $0.40 (5 GB × $0.08/GB-month, assumed 1-mo dev cycle)
- **Data Transfer:** $0.00
- **Total Training Cost:** **$1.72**

### 2. INFERENCE COSTS (Monthly, Medium Scenario)

_Based on 10,000 requests/day_

- **Compute:** $61.20 (720 hours × $0.085/hour)
- **Storage:** $0.40 (5.02 GB × $0.08/GB-month)
- **Data Transfer:** $0.00 (negligible cost for 1.4GB egress)
- **Load Balancer:** $16.20 (720 hours \* $0.0225/hr)
- **Total Monthly Cost:** **$77.80**

---

## KEY METRICS

_(Medium Scenario: 10,000 req/day)_

- **Cost per Inference:** $0.000259 (or **0.026 cents**)
- **Inferences per Dollar:** 3,856
- **Monthly Inference Capacity:** ~53.2 Billion (Single instance @ 49ms latency)

---

## SCALING SCENARIOS

| Scenario | Requests/Day | Total Monthly Cost ($) | Cost per 1000 Inferences ($) |
| :------- | :----------- | :--------------------- | :--------------------------- |
| Low      | 100          | 77.80                  | 25.93                        |
| Medium   | 10,000       | 77.80                  | 0.26                         |
| High     | 1,000,000    | 77.94                  | 0.03                         |

_(Note: Total cost is stable as it's dominated by the 24/7 fixed cost of the instance. Data transfer cost only becomes a factor at massive scale.)_

---

## VISUALIZATIONS

### Cost Breakdown (Medium Scenario)

\*\*

(Based on `cost_model.py` output: Compute is 78.7%, Load Balancer is 20.8%, Storage/Data is < 1%)

### Cost vs. Request Volume

\*\*

(This would be a line chart showing a nearly flat line, as the fixed instance cost dominates the variable request cost in all scenarios).

---

## ASSUMPTIONS

- Used On-Demand pricing for inference (`c5.large`) and training (`g4dn.xlarge`).
- Inference instance (`c5.large`) runs 24/7/30.
- Assumed average API request/response size is 5 KB for data transfer calculations.
- Assumed 5 GB of persistent storage for logs, in addition to the model file.
- Average inference time of ~49ms based on local benchmarking.

---

## OPTIMIZATION RECOMMENDATIONS

1.  **Use Spot Instances for Training:** The one-time training cost can be reduced by ~70% (from $1.32 to ~$0.40) by using `g4dn.xlarge` Spot Instances, as training is a fault-tolerant workload.
2.  **Use Reserved Instances for Inference:** For a 1-year production commitment, a Reserved Instance for the `c5.large` could reduce the main compute cost by ~40%, saving ~$24/month.
3.  **Explore Serverless:** For the "Low" traffic scenario, a 24/7 instance is wasteful. Moving to a serverless solution (like **AWS Lambda**) would be far cheaper, as we would only pay per-request. The "Cost per Inference" would be higher, but the "Total Monthly Cost" would be near $0.

---

## COST COMPARISON AND JUSTIFICATION

### Alternative: BERT (Transformer Model)

- **Cost Difference:** A base-sized BERT model is ~110M parameters, compared to our ~5M. This is >20x larger. Model size (400MB+) and memory (8GB+ RAM) would require a larger instance (e.g., `c5.xlarge` at $0.17/hr), **doubling our monthly compute cost**. Inference latency would also be higher.
- **Performance Difference:** BERT would likely provide +10-15% accuracy and understand context far better.
- **Trade-off:** Our LSTM model is **highly cost-effective**. It provides acceptable "next word" performance for a fraction of the cost of a larger Transformer.

### Justification

The current RNN approach is **cost-effective for its purpose**. The primary cost is the fixed $77.80/month for the 24/7 instance and load balancer. This cost supports over 50 billion inferences, meaning we are _significantly_ under-utilizing the server at 10,000 requests/day.

**Conclusion:** For the medium-volume scenario, the current setup is viable. For the low-volume scenario, we should **move to a serverless (AWS Lambda) deployment** to eliminate the fixed cost. If accuracy becomes a key business driver, we can explore upgrading to a larger model (like BERT) and accept the 2x cost increase.
